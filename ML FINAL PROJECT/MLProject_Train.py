# -*- coding: utf-8 -*-
"""MLPROJECT_TRAIN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uwFjJbfGsVVaG-1SUVYyVljtIzIC9EEK

Connecting to Google Drive.
"""

from google.colab import drive
drive.mount('/content/drive/')

"""Importing the required Libraries"""

import pandas as pd
import numpy as np
import scipy as sc
import math
import sklearn as sk
import matplotlib.pyplot as plt
import seaborn as sns
import random
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier

"""Reading the train file and storing it into Dataframe."""

df=pd.read_csv('drive/MyDrive/Colab Notebooks/Its_A_Fraud/train.csv')

"""Getting the information of Dataframe df using head() function"""

df.head()

"""Getting the size of dataframe"""

df.shape

"""# **EXPLORATORY DATA ANALYSIS**

Plotting the Countplot of Target Variable "isFraud".
"""

ax = sns.countplot(df['isFraud'])
plt.title('isFraud Data', fontsize=20)
for p in ax.patches:
    percentage = '{:.2f}%'.format(100 * p.get_height()/float(len(df)))
    x = p.get_x() + p.get_width()/2.
    y = p.get_height() + 3
    ax.annotate(percentage, (x, y),ha='center', fontsize = 10)
plt.show()

"""Checking the count of numerical features and categorical features"""

c_features=df.select_dtypes(include=np.object).columns
n_features=df.select_dtypes(include=np.number).columns
print(len(c_features))
print(len(n_features))

"""Plotting the Distribution for Categorical features "ProductCD" with Target Feature."""

crs = pd.crosstab(df['ProductCD'], df['isFraud'], normalize='index') * 100
crs = crs.reset_index()
crs.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)

plt.figure(figsize=(14,10))
plt.suptitle('ProductCD Distributions', fontsize=22)

plt.subplot(221)
grph = sns.countplot(x='ProductCD', data=df)


grph.set_title("ProductCD Distribution", fontsize=19)
grph.set_xlabel("ProductCD Name", fontsize=17)
grph.set_ylabel("Number of transactions", fontsize=17)
grph.set_ylim(0,400000)
for i in grph.patches:
    height = i.get_height()
    grph.text(i.get_x()+i.get_width()/2., height + 3, '{:1.2f}%'.format(height/len(df)*100), ha="center", fontsize=12) 

plt.subplot(222)
cplt = sns.countplot(x='ProductCD', hue='isFraud', data=df)
plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])
line = cplt.twinx() #creates twin axes
line = sns.pointplot(x='ProductCD', y='Fraud', data=crs, color='black', order=["W","H","C", "S", "R"])
line.set_ylabel("% of Fraud Transactions", fontsize=16)

cplt.set_title("Product CD by Target(isFraud)", fontsize=19)
cplt.set_xlabel("ProductCD category", fontsize=17)
cplt.set_ylabel("Number of transactions", fontsize=17)

"""Plotting the Card4 Distribution with Target Feature(isFraud)"""

crs = pd.crosstab(df['card4'], df['isFraud'], normalize='index') * 100
crs = crs.reset_index()
crs.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)

plt.figure(figsize=(10,6))


grph = sns.countplot(x='card4', data=df)
# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])
grph.set_title("Card4 Distribution", fontsize=19)
grph.set_ylim(0,350000)
grph.set_xlabel("Card4 Category", fontsize=17)
grph.set_ylabel("Number of transaction", fontsize=17)
for i in grph.patches:
    height = i.get_height()
    grph.text(i.get_x()+i.get_width()/2., height + 3, '{:1.2f}%'.format(height/len(df)*100), ha="center",fontsize=14) 

plt.show()

"""Plotting the Card6 Distribution with Target Feature(isFraud)."""

crs = pd.crosstab(df['card6'], df['isFraud'], normalize='index') * 100
crs = crs.reset_index()
crs.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)

plt.figure(figsize=(10,6))


g = sns.countplot(x='card6', data=df)
# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])
g.set_title("Card6 Distribution", fontsize=19)
g.set_ylim(0,350000)
g.set_xlabel("Card6 Category Names", fontsize=17)
g.set_ylabel("Count", fontsize=17)
for p in g.patches:
    height = p.get_height()
    g.text(p.get_x()+p.get_width()/2.,
            height + 3,
            '{:1.2f}%'.format(height/len(df)*100),
            ha="center",fontsize=14) 
    
plt.show()

"""Creating a temporary copy of dataframe for working on different factors."""

temp = df.copy() # df is kept aside as original copy

"""DROPPING COLUMNS WITH MORE THAN 40% MISSING VALUES"""

min_count =  int(((100-40)/100)*temp.shape[0] + 1)
temp = temp.dropna(axis = 1, thresh = min_count)
temp = temp.drop_duplicates()

"""Calculating the count of numerical features and categorical features after droppping the columns"""

n_features= temp.select_dtypes(include=np.number).columns
c_features = temp.select_dtypes(include=np.object).columns
print(len(c_features))
print(len(n_features))

"""Storing numerical and categorical features in different dataframes"""

temp_numerical = temp[n_features]
temp_categorical = temp[c_features]

"""Checking number of null values"""

temp_numerical.isna().sum()

"""Verifying unique values of the categorical features"""

for i in temp[c_features]:
  print("{} = {}".format(i, len(temp[i].unique())))

"""Verifying null values of the categorical features"""

for i in temp[c_features]:
  print("{} = {}".format(i, temp[i].isna().sum()))

"""Mapping for reducing the number of unique values in "P_emaildomain" column"""

map = {
    "netzero.net"      : "netzero",    "yahoo.co.jp"     : "yahoo",
    "prodigy.net.mx"   : "prodigy",    "windstream.net"  : "windstream",
    "outlook.es"       : "outlook",    "embarqmail.com"  : "centurylink",
    "charter.net"      : "charter",    "gmx.de"          : "gmx",
    "mail.com"         : "mail",       "centurylink.net" : "centurylink",
    "cableone.net"     : "cableone",   "hotmail.fr"      : "outlook",
    "sbcglobal.net"    : "yahoo",      "frontier.com"    : "frontier",
    "anonymous.com"    : "anonymous",  "yahoo.fr"        : "yahoo",
    "outlook.com"      : "outlook",    "live.com.mx"     : "outlook",
    "ymail.com"        : "yahoo",      "frontiernet.net" : "frontiernet",
    "cfl.rr.com"       : "spectrum",   "live.fr"         : "outlook",
    "hotmail.com"      : "outlook",    "cox.net"         : "cox",
    "hotmail.es"       : "outlook",    "aol.com"         : "aol",
    "msn.com"          : "microsoft",  "suddenlink.net"  : "suddenlink",
    "gmail.com"        : "google",     "protonmail.com"  : "proton",
    "roadrunner.com"   : "roadrunner", "web.de"          : "web.de",
    "gmail"            : "google",     "netzero.com"     : "netzero",
    "live.com"         : "outlook",    "icloud.com"      : "apple",
    "comcast.net"      : "comcast",    "hotmail.co.uk"   : "outlook",
    "yahoo.co.uk"      : "yahoo",      "att.net"         : "yahoo",
    "optonline.net"    : "optimum",    "sc.rr.com"       : "spectrum",
    "yahoo.com"        : "yahoo",      "verizon.net"     : "verizon",
    "servicios-ta.com" : "ta",         "bellsouth.net"   : "bellsouth",
    "hotmail.de"       : "outlook",    "twc.com"         : "spectrum", 
    "q.com"            : "qcom",       "rocketmail.com"  : "rocketmail",
    "juno.com"         : "juno",       "mac.com"         : "apple",
    "yahoo.com.mx"     : "yahoo",      "earthlink.net"   : "earthlink",
    "aim.com"          : "aim",        "ptd.net"         : "pdt",
    "yahoo.de"         : "yahoo",      "yahoo.es"        : "yahoo", 
    "me.com"           : "apple",      "scranton.edu"    : "scranton"
}

"""Replacing the values in "P_emaildomain" with the map calculated above"""

temp.replace({'P_emaildomain':map}, inplace = True)

"""Replacing null values in categorical features with mode"""

for i in ['card4', 'card6', 'P_emaildomain', 'M6']:
    temp[i].fillna(temp[i].mode()[0], inplace=True)

"""Replacing null values in numerical features with mean"""

temp.fillna(temp.mean(), inplace = True)

"""Dropping features with correlation greater than 0.95"""

corr_matrix = temp.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]

# Drop features 
temp.drop(to_drop, axis=1, inplace=True)

"""Calculating number of numerical and categorical features once again"""

n_features= temp.select_dtypes(include=np.number).columns
c_features = temp.select_dtypes(include=np.object).columns
print(len(c_features))
print(len(n_features))

"""Downloading the preprocessed files"""

from google.colab import files
temp.to_csv('prepro_withcolumndeletion.csv')
files.download('prepro_withcolumndeletion.csv')